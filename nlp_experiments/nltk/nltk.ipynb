{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK tutorial\n",
    "(From https://www.nltk.org/)\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    "\n",
    "We'll talk about the following sections in this tutorial:\n",
    "\n",
    "1. Tokenizer：数据预处理，将str数据变成list of tokens\n",
    "2. Stemmer：数据预处理，将list of token变成cleaned list of tokens\n",
    "3. WordNet\n",
    "4. Tips to the assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/software/anaconda/anaconda3/envs/pytorch_env/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /Users/software/anaconda/anaconda3/envs/pytorch_env/lib/python3.7/site-packages (from nltk) (4.54.1)\n",
      "Requirement already satisfied: click in /Users/software/anaconda/anaconda3/envs/pytorch_env/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /Users/software/anaconda/anaconda3/envs/pytorch_env/lib/python3.7/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /Users/software/anaconda/anaconda3/envs/pytorch_env/lib/python3.7/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: numpy in /Users/software/anaconda/anaconda3/envs/pytorch_env/lib/python3.7/site-packages (1.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yanzheyuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yanzheyuan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # to make nltk.tokenizer works\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string.split tokenizer ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information.']\n",
      "string.split tokenizer ['Current', 'NLP', 'models', \"isn't\", 'able', 'to', 'solve', 'NLU', 'perfectly.']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Text mining is to identify useful information.\"\n",
    "text2 = \"Current NLP models isn't able to solve NLU perfectly.\"\n",
    "\n",
    "print(\"string.split tokenizer\", text1.split(\" \"))\n",
    "print(\"string.split tokenizer\", text2.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot deal with punctuations, i.e., full stops and apostrophes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular expression tokenizer ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '']\n",
      "regular expression tokenizer ['Current', 'NLP', 'models', \"isn't\", 'able', 'to', 'solve', 'NLU', 'perfectly', '']\n"
     ]
    }
   ],
   "source": [
    "import regex # regular expression 正则表达式\n",
    "print(\"regular expression tokenizer\", regex.split(\"[\\s\\.]\", text1))\n",
    "print(\"regular expression tokenizer\", regex.split(\"[\\s\\.]\", text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, the `string.split` function can not deal with punctuations\n",
    "- Simple regular expression can deal with most punctuations but may fail in the cases of \"isn't, wasn't, can't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    e.g.\n",
    "    Input: 'Text mining is to identify useful information.'\n",
    "    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
      "['Current', 'NLP', 'models', 'is', \"n't\", 'able', 'to', 'solve', 'NLU', 'perfectly', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(text1))\n",
    "print(tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob', \"'s\", 'text', 'mining', 'skills', 'are', 'perfect', '.']\n",
      "['Bob', \"'s\", 'text', 'mining', 'skills', '(', 'or', ',', 'NLP', ')', 'are', 'perfect', '.']\n",
      "['Bob', \"'s\", 'text', 'mining', 'skills', 'are', 'perfect', '...']\n"
     ]
    }
   ],
   "source": [
    "# Other examples:\n",
    "# 1. Possessive cases: Apostrophe (isn't, I've, ...) 所有格\n",
    "tokens = tokenize(\"Bob's text mining skills are perfect.\")\n",
    "print(tokens)\n",
    "# 2. Parentheses 插入语\n",
    "tokens = tokenize(\"Bob's text mining skills (or, NLP) are perfect.\")\n",
    "print(tokens)\n",
    "# 3. ellipsis 省略号\n",
    "tokens = tokenize(\"Bob's text mining skills are perfect...\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stemming and lemmatization\n",
    "\n",
    "(https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "Stemming: chops off the ends of words to acquire the root, and often includes the removal of derivational affixes. \n",
    "\n",
    "e.g., gone -> go, wanted -> want, trees -> tree.\n",
    "\n",
    "Lemmatization: doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . \n",
    "\n",
    "Differences:\n",
    "The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma (focus on the concrete semantic meaning). \n",
    "\n",
    "E.g.: useful -> use(stemming), useful(lemmatization)\n",
    "\n",
    "PorterStemmer:\n",
    "\n",
    "Rule-based methods. E.g., SSES->SS, IES->I, NOUNS->NOUN. # misses->miss, flies->fli.\n",
    "\n",
    "Doc: https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "- stemming: 词干提取，指去除单词前后缀返回词根的过程，比如playing-play\n",
    "- lemmatization: 词形还原，指根据词典知识还原词原型，比如drove-drive\n",
    "- PorterStemmer: 基于一些规则的stem，指定如何去除前后缀。来源于nltk.stem\n",
    "- WordNetLemmatizer: 来源于nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = stem(tokenize(\"Text mining is to identify useful information.\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "def lemmatize(tokens):\n",
    "    return [lm.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = lemmatize(tokenize(\"Text mining is to identify useful information.\"))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self practice\n",
    "- lemmatize 和 stem 是可以有叠加效果的，但貌似在工业界预处理过程是\n",
    "  - tokenize\n",
    "  - lemmatize/stem\n",
    "  - stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'minings', 'is', 'to', 'identify', 'useful', 'informations', '.', 'lighted', 'up', 'as', 'if', 'you', 'had', 'a', 'chooses', ',', 'wolves']\n",
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.', 'light', 'up', 'as', 'if', 'you', 'had', 'a', 'choos', ',', 'wolv']\n",
      "['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.', 'lighted', 'up', 'a', 'if', 'you', 'had', 'a', 'chooses', ',', 'wolf']\n",
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.', 'light', 'up', 'a', 'if', 'you', 'had', 'a', 'choos', ',', 'wolf']\n",
      "['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.', 'light', 'up', 'a', 'if', 'you', 'had', 'a', 'choos', ',', 'wolv']\n"
     ]
    }
   ],
   "source": [
    "# test on the combination of lemmatization and stemming\n",
    "import nltk \n",
    "tokens = nltk.word_tokenize('Text minings is to identify useful informations. lighted up as if you had a chooses, wolves')\n",
    "print(tokens)\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "ps_test = PorterStemmer()\n",
    "lm_test = WordNetLemmatizer()\n",
    "ps_test_1 = PorterStemmer()\n",
    "lm_test_1 = WordNetLemmatizer()\n",
    "def stem(ps_test,tokens):\n",
    "    return [ps_test.stem(token) for token in tokens]\n",
    "def lemmatize(lm_test,tokens):\n",
    "    return [lm_test.lemmatize(token) for token in tokens]\n",
    "print(stem(ps_test,tokens))\n",
    "print(lemmatize(lm_test,tokens))\n",
    "print(stem(ps_test,lemmatize(lm_test,tokens)))\n",
    "print(lemmatize(lm_test_1,stem(ps_test_1,tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. WordNet\n",
    "\n",
    "https://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "- a semantically-oriented dictionary of English,\n",
    "- similar to a traditional thesaurus but with a richer structure\n",
    "\n",
    "- 一个基于词语义的字典\n",
    "- 同义词，词的区别\n",
    "\n",
    "- 应用\n",
    "  - 输出同义词\n",
    "  - wn.synsets实例的方法\n",
    "    - defination, examples, lemma_names等\n",
    "    - 输出一个词的上位/下位词\n",
    "      - 一个词的root词\n",
    "      - 输出两个词相同的上位/下位词\n",
    "    - 两个实例的相似度\n",
    "      - path_similarity: shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy.\n",
    "      - lch_similarity: Leacock-Chodorow Similarity, above + the maximum depth of the taxonomy.\n",
    "      - wup_similarity: Wu-Palmer Similarity, based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer.\n",
    "      - res_similarity: Resnik Similarity, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 synsets\n",
    "\n",
    "A set of one or more **synonyms** that are interchangeable in some context without changing the truth value of the proposition in which they are embedded.\n",
    "\n",
    "- 同义词，在某些上下文中可以替换且不改变原来语义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up a word using synsets(); \n",
    "wn.synsets('dog')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10'),\n",
       " Synset('bank.v.01'),\n",
       " Synset('bank.v.02'),\n",
       " Synset('bank.v.03'),\n",
       " Synset('bank.v.04'),\n",
       " Synset('bank.v.05'),\n",
       " Synset('deposit.v.02'),\n",
       " Synset('bank.v.07'),\n",
       " Synset('trust.v.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synset \t definition\n",
      "Synset('bank.n.01') \t sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') \t a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') \t a long ridge or pile\n",
      "Synset('bank.n.04') \t an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') \t a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') \t the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') \t a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') \t a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') \t a building in which the business of banking transacted\n",
      "Synset('bank.n.10') \t a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') \t tip laterally\n",
      "Synset('bank.v.02') \t enclose with a bank\n",
      "Synset('bank.v.03') \t do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') \t act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') \t be in the banking business\n",
      "Synset('deposit.v.02') \t put into a bank account\n",
      "Synset('bank.v.07') \t cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') \t have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "print(\"synset\",\"\\t\",\"definition\")\n",
    "for synset in wn.synsets('bank'):\n",
    "    print(synset, '\\t', synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function has an optional pos argument which lets you constrain the part of speech of the word:\n",
    "# pos: part-of-speech\n",
    "wn.synsets('bank', pos=wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('dog.n.01')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the dog barked all night']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'domestic_dog', 'Canis_familiaris']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dir() 函数不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时，返回参数的属性、方法列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_all_hypernyms',\n",
       " '_definition',\n",
       " '_examples',\n",
       " '_frame_ids',\n",
       " '_hypernyms',\n",
       " '_instance_hypernyms',\n",
       " '_iter_hypernym_lists',\n",
       " '_lemma_names',\n",
       " '_lemma_pointers',\n",
       " '_lemmas',\n",
       " '_lexname',\n",
       " '_max_depth',\n",
       " '_min_depth',\n",
       " '_name',\n",
       " '_needs_root',\n",
       " '_offset',\n",
       " '_pointers',\n",
       " '_pos',\n",
       " '_related',\n",
       " '_shortest_hypernym_paths',\n",
       " '_wordnet_corpus_reader',\n",
       " 'also_sees',\n",
       " 'attributes',\n",
       " 'causes',\n",
       " 'closure',\n",
       " 'common_hypernyms',\n",
       " 'definition',\n",
       " 'entailments',\n",
       " 'examples',\n",
       " 'frame_ids',\n",
       " 'hypernym_distances',\n",
       " 'hypernym_paths',\n",
       " 'hypernyms',\n",
       " 'hyponyms',\n",
       " 'in_region_domains',\n",
       " 'in_topic_domains',\n",
       " 'in_usage_domains',\n",
       " 'instance_hypernyms',\n",
       " 'instance_hyponyms',\n",
       " 'jcn_similarity',\n",
       " 'lch_similarity',\n",
       " 'lemma_names',\n",
       " 'lemmas',\n",
       " 'lexname',\n",
       " 'lin_similarity',\n",
       " 'lowest_common_hypernyms',\n",
       " 'max_depth',\n",
       " 'member_holonyms',\n",
       " 'member_meronyms',\n",
       " 'min_depth',\n",
       " 'name',\n",
       " 'offset',\n",
       " 'part_holonyms',\n",
       " 'part_meronyms',\n",
       " 'path_similarity',\n",
       " 'pos',\n",
       " 'region_domains',\n",
       " 'res_similarity',\n",
       " 'root_hypernyms',\n",
       " 'shortest_path_distance',\n",
       " 'similar_tos',\n",
       " 'substance_holonyms',\n",
       " 'substance_meronyms',\n",
       " 'topic_domains',\n",
       " 'tree',\n",
       " 'usage_domains',\n",
       " 'verb_groups',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn.synset('dog.n.01'))\n",
    "# isA: hyponyms, hypernyms：下位词，上位词\n",
    "# part_of: member_holonyms, substance_holonyms, part_holonyms\n",
    "# being part of: member_meronyms, substance_meronyms, part_meronyms\n",
    "# domains: topic_domains, region_domains, usage_domains\n",
    "# attribute: attributes\n",
    "# entailments: entailments\n",
    "# causes: causes\n",
    "# also_sees: also_sees\n",
    "# verb_groups: verb_groups\n",
    "# similar_to: similar_tos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check more relations in http://www.nltk.org/api/nltk.corpus.reader.html?highlight=wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypernyms: [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "hyponyms: [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# hypernyms: abstraction 上层抽象\n",
    "# hyponyms: instantiation 下层具象\n",
    "\n",
    "dog = wn.synset('dog.n.01')\n",
    "print(\"hypernyms:\", dog.hypernyms())\n",
    "print(\"hyponyms:\", dog.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('carnivore.n.01')]\n",
      "[Synset('placental.n.01')]\n",
      "[Synset('mammal.n.01')]\n",
      "root hypernyms for dog: [Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(dog.hypernyms()[0].hypernyms()) # the hypernym of canine\n",
    "# animals that feeds on flesh\n",
    "print(dog.hypernyms()[0].hypernyms()[0].hypernyms()) # the hypernym of carnivore\n",
    "# placental mammals\n",
    "print(dog.hypernyms()[0].hypernyms()[0].hypernyms()[0].hypernyms()) # the hypernym of placental\n",
    "# mammals\n",
    "# ...\n",
    "print(\"root hypernyms for dog:\", dog.root_hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root hypernyms for cat: [Synset('feline.n.01')]\n",
      "root hypernyms for cat: [Synset('entity.n.01')]\n",
      "the lowest common hypernyms of dog and cat\n",
      "[Synset('carnivore.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# find common hypernyms\n",
    "print(\"root hypernyms for cat:\", wn.synset('cat.n.01').hypernyms())\n",
    "print(\"root hypernyms for cat:\", wn.synset('cat.n.01').root_hypernyms())\n",
    "print(\"the lowest common hypernyms of dog and cat\")\n",
    "print(wn.synset('dog.n.01').lowest_common_hypernyms(wn.synset('cat.n.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "corgi = wn.synset('corgi.n.01')\n",
    "bensenji = wn.synset('basenji.n.01')\n",
    "cat = wn.synset('cat.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat) # dog <- canine <- carnivore -> feline -> cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(corgi) # corgi <- dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corgi.path_similarity(bensenji) # bensenji <- dog -> corgi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = wn.synset('hit.v.01')\n",
    "slap = wn.synset('slap.v.01')\n",
    "jump = wn.synset('jump.v.01')\n",
    "run = wn.synset('run.v.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(slap) # 1/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit.path_similarity(jump) # 1/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also check:\n",
    "- wup_similarity\n",
    "- lch_similarity\n",
    "- res_similarity\n",
    "...\n",
    "\n",
    "Find more on https://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Traverse the synsets to build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_graph_hypernyms = {}\n",
    "# or you could use networkx package\n",
    "\n",
    "for synset in list(wn.all_synsets('n'))[:10]:\n",
    "    for hyp_syn in synset.hypernyms():\n",
    "        wn_graph_hypernyms[synset.name()] = {**wn_graph_hypernyms.get(synset.name(), {}), **{hyp_syn.name():True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_graph_hypernyms['physical_entity.n.01']['entity.n.01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tips to the assignments\n",
    "\n",
    "Some corpus in the NLTK.\n",
    "\n",
    "Reference: https://www.nltk.org/book/ch02.html. You could search for `gutenberg` and `brown` for detailed documentations.\n",
    "\n",
    "### 4.1 gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/yanzheyuan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg as gb\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = 'austen-sense.txt'\n",
    "word_list = gb.words(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']', 'CHAPTER', '1', 'The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.', 'Their', 'estate', 'was', 'large', ',', 'and', 'their', 'residence', 'was', 'at', 'Norland', 'Park', ',', 'in', 'the', 'centre', 'of', 'their', 'property', ',', 'where', ',', 'for', 'many', 'generations', ',', 'they', 'had', 'lived', 'in', 'so', 'respectable', 'a', 'manner', 'as', 'to', 'engage', 'the', 'general', 'good', 'opinion', 'of', 'their', 'surrounding', 'acquaintance', '.', 'The', 'late', 'owner', 'of', 'this', 'estate', 'was', 'a', 'single', 'man', ',', 'who', 'lived', 'to', 'a', 'very', 'advanced', 'age', ',', 'and', 'who', 'for', 'many', 'years', 'of', 'his', 'life', ',', 'had', 'a', 'constant', 'companion']\n"
     ]
    }
   ],
   "source": [
    "print(word_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = gb.sents(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/yanzheyuan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download(\"brown\")\n",
    "print(brown.categories())\n",
    "\n",
    "romance_word_list = brown.words(categories='romance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', 'neither', 'liked', 'nor', 'disliked', 'the', ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romance_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
