{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import d2lzh_pytorch as d2dl\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstmModel(\n",
      "  (lstm_layer): LSTM(1447, 256)\n",
      "  (dense_layer): Linear(in_features=256, out_features=1447, bias=True)\n",
      ")\n",
      "Epoch: 50, Perplexity: 1.044233, Time: 2.97 sec\n",
      "- 分开始没人注意到你我 等雨变强之前 我们将会分化软弱 趁时间没发觉 让我带着你离开 没有了证明 没有了空\n",
      "- 不分开 为什么还要我用微笑来带过 我没有这种天份 包容你也接受他 不用担心的太多 我会一直好好过 你已经远\n",
      "Epoch: 100, Perplexity: 1.575040, Time: 3.43 sec\n",
      "- 分开始脸太多难过 你比我好想就这样牵着你 微笑起天都能回到  为什么这么简单你做不到  但是一个人之后 \n",
      "- 不分开 为什么这种速度你追不要问 我就是那条龙 渴望着血脉相通 无限个千万弟兄 我把天 挑落好不再也能说你\n",
      "Epoch: 150, Perplexity: 1.016938, Time: 3.09 sec\n",
      "- 分开都迁就着你 我真的没有天份 安静的没这么快 我会学着放弃你 是因为我太爱你 是因为我太爱你你要我说多\n",
      "- 不分开 它在空中停留 所有人看着我 抛物线进球 单手过人运球 篮下妙传出手 漂亮的假动作 帅呆了我 全场盯\n",
      "Epoch: 200, Perplexity: 1.019268, Time: 3.76 sec\n",
      "- 分开 爱像一阵风 吹完它就走 这样的节奏 谁都无可奈何 没有你以后 我灵魂失控 黑云在降落 我被它拖着走\n",
      "- 不分开 为什么还要我用微笑来带过 我没有这种天份 包容你也接受他 不用担心的太多 我会一直好好过 你已经远\n",
      "Epoch: 250, Perplexity: 1.015867, Time: 3.41 sec\n",
      "- 分开都迁就着你 我真的没有天份 安静的没这么快 我会学着放弃你 是因为我太爱你 是因为我太爱你 该不该搁\n",
      "- 不分开 为什么这样子 你看着我说你已经决定 我拉不住你 他的手应该比我更暖 铁盒的序变成了日记 变成了空气\n",
      "Epoch: 300, Perplexity: 1.025433, Time: 3.31 sec\n",
      "- 分开时间 所以你弃权 所以你弃权 我想要的想做的你比谁都了  你想说的想给的我全都知道 未接来电没留言 \n",
      "- 不分开 为什么还要我用微笑来带过 我没有这种天份 包容你也接受他 不用担心的太多 我会一直好好过 你已经远\n"
     ]
    }
   ],
   "source": [
    "# Obtain Data\n",
    "def load_jaychou_lyrics():\n",
    "    with open('jaychou_lyrics.txt') as f:\n",
    "        corpus_char = f.read()\n",
    "    corpus_char = corpus_char.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_char = corpus_char[0:20000]\n",
    "    char_list = list(set(corpus_char))\n",
    "    vocab_size = len(char_list)\n",
    "    char_dict = dict([(item,i) for i, item in enumerate(char_list)])\n",
    "    corpus_indices = [char_dict[item] for item in corpus_char]\n",
    "    return corpus_indices, char_dict, char_list, vocab_size\n",
    "\n",
    "(corpus_indices, char_dict, char_list, vocab_size) = load_jaychou_lyrics()\n",
    "input_size, hidden_size = vocab_size, 256\n",
    "\n",
    "# Load Data\n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=device):\n",
    "    corpus_indices = torch.tensor(corpus_indices, dtype=torch.float32, device=device)\n",
    "    len_corpus = len(corpus_indices)\n",
    "    num_batch = len_corpus // batch_size\n",
    "    indices = corpus_indices[:num_batch*batch_size].view(batch_size, num_batch)\n",
    "    num_batch_example = (num_batch-1) // num_steps\n",
    "    for i in range(num_batch_example):\n",
    "        i = i * num_steps\n",
    "        X = indices[:,i:i+num_steps]\n",
    "        Y = indices[:,i+1:i+1+num_steps]\n",
    "        yield X,Y\n",
    "\n",
    "# Define Model\n",
    "class lstmModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(lstmModel,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.lstm_layer = nn.LSTM(input_size,hidden_size,bidirectional=False)\n",
    "        self.hidden_size = self.lstm_layer.hidden_size * (2 if self.lstm_layer.bidirectional else 1)\n",
    "        self.dense_layer = nn.Linear(self.hidden_size, self.input_size)\n",
    "        self.state = None  # in LSTM state=(h,c)\n",
    "    def forward(self, inputs, state):\n",
    "        x = d2dl.to_onehot(inputs, self.input_size)\n",
    "        y, self.state = self.lstm_layer(torch.stack(x),state)  # stack\n",
    "        output = self.dense_layer(y.view(-1,y.shape[-1]))  # view\n",
    "        return output, self.state\n",
    "\n",
    "net = lstmModel(input_size,hidden_size).to(device)\n",
    "print(net)\n",
    "        \n",
    "# Predict Model\n",
    "def predict_lstm_lm(prefix, net, pred_len, char_dict, char_list, vocab_size, device):\n",
    "    outputs = [char_dict[prefix[0]]]\n",
    "    state = None\n",
    "    for i in range(pred_len+len(prefix)-1):\n",
    "        inputs = torch.tensor([outputs[-1]], device=device).view(1,1)\n",
    "        if state is not None:\n",
    "            if isinstance(state, tuple): # LSTM, state:(h, c)  \n",
    "                state = (state[0].to(device), state[1].to(device))\n",
    "            else:   \n",
    "                state = state.to(device) \n",
    "        (Y, state) = net(inputs, state)\n",
    "        if i < len(prefix)-1:\n",
    "            outputs.append(char_dict[prefix[i+1]])\n",
    "        else:\n",
    "            outputs.append(int(Y.argmax(1).item()))\n",
    "    return ''.join([char_list[item] for item in outputs])\n",
    "    \n",
    "\n",
    "# Train Model using consecutive dataloader\n",
    "def train_lstm_lm(net, lr, num_epochs, corpus_indices, batch_size, num_steps, device,\\\n",
    "                 grad_clipping_theta, pred_period, prefixes, pred_len, vocab_size, char_dict,\\\n",
    "                 char_list):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizor = optim.Adam(net.parameters(), lr)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        state = None\n",
    "        l_sum, n, start_time = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, device)\n",
    "        for X, Y in data_iter:\n",
    "            if state is not None:\n",
    "                if isinstance(state, tuple):\n",
    "                    state = (state[0].detach(),state[1].detach())\n",
    "                else: \n",
    "                    state = state.detach()\n",
    "            \n",
    "            # modeling\n",
    "            (outputs, state) = net(X, state)\n",
    "            y = torch.transpose(Y,0,1).reshape(-1)  # contiguous().view(-1)\n",
    "            l = loss(outputs, y.long())\n",
    "            \n",
    "            # bp\n",
    "            optimizor.zero_grad()\n",
    "            l.backward()\n",
    "            d2dl.grad_clipping(net.parameters(), grad_clipping_theta, device)\n",
    "            optimizor.step()\n",
    "            \n",
    "            # stats\n",
    "            l_sum += l.item()\n",
    "            n += 1\n",
    "        \n",
    "        try:\n",
    "            perplexity = math.exp(l_sum/n)\n",
    "        except OverflowError:\n",
    "            perplexity = float('inf')\n",
    "        \n",
    "        if (epoch+1) % pred_period == 0:\n",
    "            print('Epoch: %d, Perplexity: %f, Time: %.2f sec' %\\\n",
    "                  (epoch+1, perplexity, time.time()-start_time))\n",
    "            \n",
    "            for prefix in prefixes:\n",
    "                print('-', predict_lstm_lm(prefix, net, pred_len, char_dict, char_list, vocab_size, device))\n",
    "            \n",
    "            \n",
    "            \n",
    "# Hyperparameters and train\n",
    "num_epochs = 250\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "grad_clipping_theta = 0.01\n",
    "num_steps = 32\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n",
    "train_lstm_lm(net, lr, num_epochs, corpus_indices, batch_size, num_steps, device,\\\n",
    "                 grad_clipping_theta, pred_period, prefixes, pred_len, vocab_size, char_dict,\\\n",
    "                 char_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [0,1]\n",
    "res.append(1)\n",
    "res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
