{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4w5KzEWS3R8"
   },
   "source": [
    "# DenseNet with MNIST Dataset\n",
    "\n",
    "`Author: YUAN Yanzhe`\n",
    "\n",
    "- This notebook is a reproduction of the [DenseNet paper](https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html).\n",
    "  - If you want to do parameter fine-tuning, setting hyperparameters on the entrance of the model is recommended.\n",
    "    - e.g. def \\_\\_init\\_\\_(param) \n",
    "- The code runs on Google Colab, GPU mode\n",
    "\n",
    "一些细节：\n",
    "- DenseNet和ResNet最大的区别是：DenseNet里前一模块的输出不是像ResNet那样和当前模块的输出相加（y=y+identity(x)），而是在feature_num这一维度上连结(cat on dim=1)。这样模块输出可以直接传入当前模块后面的层。\n",
    "- DenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer）。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。\n",
    "  - cnn_block用ResNet改良版的“BN-激活-卷积”结构，不改变image size改变特征数。dense block由多个这样的cnn_block结构组成。\n",
    "  - dense block的输入是cnn_block的数量，输入的特征数，以及增长率（即每个cnn_block的输出feature_num，注意，这里cnn_block本体最后一个conv的输出一直是不变的（即增长率），最后需要和原输入在feature_num维度cat作为cnn block的输出以及下一个cnn block的输入，以此达到连接的效果。所以最后cnn_block的输出是输入特征数+增长率，整个一个dense block的输出就是：输入特征+cnn_block数量\\*增长率）\n",
    "  - transition block就是控制减少特征数的。主要是一个11卷积和平均池化，前者减少特征数后者减半image size。\n",
    "- Dense Net网络结构：\n",
    "  - densenet_layer\n",
    "    - 首先一个类似于resnet第一层的结构\n",
    "    - 再来4个dense block-transition_block结构\n",
    "    - 全局池化层（BN-ReLU-Pool）: 充当最后一个transition block\n",
    "  - flatten：去除最后2维。\n",
    "  - 特征维全连接层：nn.Linear(~,10)。 这里的feature_num是248，当然，可以更大\n",
    "- 和之前的模型框架不同，这里多加入了验证环节，用一个数据输入网络得出输出，用于debug。此时，模型的设计更重要，不止是需要forward里顺序对。\n",
    "``` python \n",
    "X = torch.rand((1, 1, 96, 96))\n",
    "for layer in net.children():\n",
    "    X = layer(X)\n",
    "    print(' output shape:\\t', X.shape)\n",
    "```\n",
    "  - 相对应的，由于我们用的是class方法继承nn.Module来定义类，且这里用了net.children()或者net.named_children()方法，打印的是net中继承nn.Module的第一层级（想象成一颗树结构），根据这个层级来运行net。\n",
    "    - 所以，我们需要把flatten层实现，不然无法运行\n",
    "    - 并且，不要self定义嵌入大结构的小结构，用self定义最外面的（比如sequence）结构。因为net也会按是否有继承nn.Module的子结构依次打印。如果像之前googlenet的主网络init中的定义方法一样，我们的net包含一些self.blocks，最后又将blocks串联变成一个self.block_layer，与fc和fallten等layer一起定义。这时候我们的self.blocks需要去掉self，不然用验证无法进行，会断路。\n",
    "      - 虽然但是，这样forward是会正常运行的所以用数据集进行测试的时候是无所谓的。这里为了能够验证，并且方便debug，在设计网络的时候可以注意一下。\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqfQIT7p6sAp",
    "outputId": "84a39278-c379-400e-c402-ea8ab0afb1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "URJCj5IT60-C"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/d2dl_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5ymqi9I68xf",
    "outputId": "2ddaa8b5-f296-4cf1-b4fb-8193db0ab59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0+cu101\n",
      "device on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.utils import data as Data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "\n",
    "import d2lzh_pytorch as d2dl\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device on:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRIHdv-R6_EE",
    "outputId": "eb379321-f230-4e9c-a171-c7579a10f3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denseNet(\n",
      "  (denseNet_layer): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (DenseBlock_0): denseBlock(\n",
      "        (denseBlock_list): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (TransitionBlock_0): Sequential(\n",
      "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (DenseBlock_1): denseBlock(\n",
      "        (denseBlock_list): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (TransitionBlock_1): Sequential(\n",
      "        (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(224, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (DenseBlock_2): denseBlock(\n",
      "        (denseBlock_list): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(112, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(144, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(176, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(208, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (TransitionBlock_2): Sequential(\n",
      "        (0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(240, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (DenseBlock_3): denseBlock(\n",
      "        (denseBlock_list): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(120, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(152, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(184, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (1): ReLU()\n",
      "            (2): Conv2d(216, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): globalAvgPool()\n",
      "    )\n",
      "  )\n",
      "  (flatten_layer): flattenLayer()\n",
      "  (fc_layer): Linear(in_features=248, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# network hyperparameter\n",
    "num_channels, growth_rate = 64, 32  # num_channels为当前的通道数\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "# Load Data\n",
    "# non-default argument follows default argument, has to define non-default value first\n",
    "def load_data_from_mnist(batch_size, resize=None, root=''):\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(transforms.Resize(resize))\n",
    "    trans.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(trans)\n",
    "\n",
    "    train_data = torchvision.datasets.MNIST(root=root,train=True,transform=transform,download=False)\n",
    "    test_data = torchvision.datasets.MNIST(root=root,train=False,transform=transform,download=False)\n",
    "    train_iterator = Data.DataLoader(train_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    test_iterator = Data.DataLoader(test_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "    return train_iterator, test_iterator\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=''):\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "\n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "#train_iterator, test_iterator = load_data_fashion_mnist(batch_size,resize=96)\n",
    "train_iterator, test_iterator = load_data_from_mnist(batch_size,resize=96)\n",
    "\n",
    "# Define Model\n",
    "class flattenLayer(nn.Module):\n",
    "    # this can be realized by y.view(x.shape[0],-1) in forward()\n",
    "    # however, in order to view the parameter changing over each block, we define it\n",
    "    def __init__(self):\n",
    "        super(flattenLayer,self).__init__()\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "\n",
    "class globalAvgPool(nn.Module):\n",
    "    # the function of global average pooling is to reduce the image size to (1,1),\n",
    "    # which is convenient to reduce dimension later\n",
    "    def __init__(self):\n",
    "        super(globalAvgPool,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return nn.functional.avg_pool2d(x,x.size()[2:])\n",
    "\n",
    "class denseBlock(nn.Module):\n",
    "    # DenseBlock consists of a number of (num) cnn blocks, vertically sequentially every block is denseBlock-like-connected.\n",
    "    # the image size is remained but the feature_num is accumulated through cnn blocks. \n",
    "    # in the feature_num dim the current layer is concatenate with the previous layer (f_num = f_num + f_num_previous)\n",
    "    # inside every cnn blocks, a optimized structrue for CNN block in ResNet: (bn-relu-cnn) is used.\n",
    "    def __init__(self, num_cnns, c_in, c_out):\n",
    "        super(denseBlock,self).__init__()\n",
    "        block = []\n",
    "        for i in range(num_cnns):\n",
    "            feature_in = c_in + i * c_out  \n",
    "            block.append(self.cnn_block(feature_in,c_out))  # c_in + i * c_out is accumulated \n",
    "        self.denseBlock_list = nn.ModuleList(block)\n",
    "        self.c_out = c_in + num_cnns * c_out  # record the final output feature_num in one dense block\n",
    "         \n",
    "    def cnn_block(self, c_in, c_out):\n",
    "        blk = nn.Sequential(\n",
    "            nn.BatchNorm2d(c_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c_in,c_out,kernel_size=3,padding=1)\n",
    "        )\n",
    "        return blk\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.denseBlock_list:\n",
    "            y = block(x)\n",
    "            x = torch.cat((x,y),dim=1)   # this is where dense connection happens\n",
    "        return x\n",
    "\n",
    "class denseNet(nn.Module):\n",
    "    # DenseNet consists of transition_block and denseBlock. the details is as follows:\n",
    "    # the first part is like GoogLeNet and ResNet: 7*7conv-bn-relu-pool\n",
    "    # the second part is 4 pair of denseBlock-transition_block (like 4 resnet block in ResNet)\n",
    "    # we use transition block to reduce the image size by half (ResNet uses just a residual block with stride 2)\n",
    "    # the third layer is a globalAvgPooling based layer to reduce the image size to 1*1 to replace fc(fnn).\n",
    "    # the fourth part is the linear layer to reduce feature_num and feed into softmax.\n",
    "    def __init__(self, num_channels, growth_rate, num_convs_in_dense_blocks): \n",
    "        super(denseNet,self).__init__()\n",
    "        block_1 = nn.Sequential(  # just like ResNet\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        block_2 = nn.Sequential()\n",
    "        for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "            dense_block = denseBlock(num_convs,num_channels,growth_rate)\n",
    "            block_2.add_module('DenseBlock_%d' %i, dense_block)  # add dense block to increase features\n",
    "            num_channels = dense_block.c_out  # record the final output feature_num in one dense block\n",
    "            #print('num_channels:',num_channels)\n",
    "            \n",
    "            if i != (len(num_convs_in_dense_blocks)-1):\n",
    "                block_2.add_module('TransitionBlock_%d' %i, self.transitions_block(num_channels,num_channels//2))  # add transition block to reduce features. \n",
    "                num_channels = num_channels//2\n",
    "        global_pool_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_channels),\n",
    "            nn.ReLU(),\n",
    "            globalAvgPool(),\n",
    "        )\n",
    "        self.denseNet_layer = nn.Sequential(block_1,block_2,global_pool_block)\n",
    "        self.flatten_layer = flattenLayer()\n",
    "        self.fc_layer = nn.Linear(num_channels,10)\n",
    "    \n",
    "    def transitions_block(self, c_in, c_out):\n",
    "        blk = nn.Sequential(\n",
    "            nn.BatchNorm2d(c_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c_in,c_out,kernel_size=1),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2),  # reduce the image size by half\n",
    "        )\n",
    "        return blk\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.denseNet_layer(x)\n",
    "        y = self.flatten_layer(y)\n",
    "        y = self.fc_layer(y)  \n",
    "        return y\n",
    "\n",
    "net = denseNet(num_channels,growth_rate,num_convs_in_dense_blocks)\n",
    "print(net)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizor = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lqoXHIhn7A8T",
    "outputId": "982ccff3-2eb4-47c4-910e-a9bba76d1b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " output shape:\t torch.Size([1, 248, 1, 1])\n",
      " output shape:\t torch.Size([1, 248])\n",
      " output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the network\n",
    "X = torch.rand((1, 1, 96, 96))\n",
    "for layer in net.children():\n",
    "    X = layer(X)\n",
    "    print(' output shape:\\t', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdvY8qOgIrk8",
    "outputId": "a2088ca7-4351-449d-e4ad-edab971c3069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cuda\n",
      "testing on: cuda\n",
      "Epoch: 1, Average loss: 0.1517, Average accuracy: 96.65%, Test Accuracy: 98.46%, time: 35.8sec\n",
      "testing on: cuda\n",
      "Epoch: 2, Average loss: 0.0360, Average accuracy: 98.88%, Test Accuracy: 99.13%, time: 35.9sec\n",
      "testing on: cuda\n",
      "Epoch: 3, Average loss: 0.0273, Average accuracy: 99.18%, Test Accuracy: 98.35%, time: 35.7sec\n",
      "testing on: cuda\n",
      "Epoch: 4, Average loss: 0.0210, Average accuracy: 99.36%, Test Accuracy: 98.90%, time: 35.9sec\n",
      "testing on: cuda\n",
      "Epoch: 5, Average loss: 0.0199, Average accuracy: 99.40%, Test Accuracy: 99.22%, time: 35.8sec\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "def evaluate_model(net, test_iterator, device):\n",
    "    net = net.to(device)\n",
    "    print('testing on:', device)\n",
    "    with torch.no_grad():\n",
    "        correct,num_exp = 0.0,0\n",
    "        for X,y in test_iterator:\n",
    "            if isinstance(net, nn.Module):\n",
    "                net.eval()  # eval mode will shut off dropout function\n",
    "                correct += (net(X.to(device)).argmax(1)==y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else: \n",
    "                print('is this your self-defined nn module?? we are not considering GPU if so')\n",
    "                if('is_training' in net.__code__.co_varnames): \n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            num_exp += y.size(0)\n",
    "     \n",
    "    return correct/num_exp*100\n",
    "\n",
    "def train_model(num_epochs, train_iterator, test_iterator, loss_func, optimizor, net, device):\n",
    "    net = net.to(device)\n",
    "    print('training on:', device)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss,total_batch,total_acc,total_num,start_time = 0.0,0,0.0,0,time.time()\n",
    "        for X, y in train_iterator:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = net(X)\n",
    "            loss = loss_func(output,y)\n",
    "            optimizor.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizor.step()\n",
    "            \n",
    "            total_loss += loss.cpu().item()\n",
    "            total_batch += 1\n",
    "            total_acc += (output.argmax(1)==y).sum().cpu().item()\n",
    "            total_num += y.size(0)\n",
    "        \n",
    "        test_acc = evaluate_model(net, test_iterator, device)\n",
    "        print('Epoch: {}, Average loss: {:.4f}, Average accuracy: {:.2f}%, Test Accuracy: {:.2f}%, time: {:.1f}sec' \\\n",
    "              .format(epoch+1, total_loss/total_batch, total_acc/total_num*100, test_acc, time.time()-start_time))\n",
    "\n",
    "train_model(num_epochs,train_iterator,test_iterator,loss_func,optimizor,net,device)\n",
    "        \n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "rz497ZUHJLyx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GauylqdfNLsM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "densenet_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
