{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps_opaQN4kb4"
   },
   "source": [
    "# ResNet with MNIST Dataset\n",
    "`Author: YUAN Yanzhe`\n",
    "\n",
    "- This notebook is a reproduction of the [ResNet paper](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).\n",
    "  - If you want to do parameter fine-tuning, setting hyperparameters on the entrance of the model is recommended.\n",
    "    - e.g. def \\_\\_init\\_\\_(param) \n",
    "- The code runs on Google Colab, GPU mode\n",
    "\n",
    "一些细节：\n",
    "- ResNet的motivation：对神经网络添加新的layer，可能会使训练误差减少。\n",
    "  - BN是一种思路，但实践中，添加BN看似并不能完全解决问题。\n",
    "  - 是否能添加上一层（块）的输出到这一层（块）的输出上，这样能避免信息遗忘，添加一条通路也可以缓解梯度消失等问题。\n",
    "  - 因为原网络解的空间只是新模型解的空间的子空间，如果我们能将新添加的层训练成恒等映射f(x)=x，新模型和原模型将同样有效。\n",
    "- ResNet的结构：\n",
    "  - 首先一个类似于googlenet第一层的结构\n",
    "  - 然后是4个resnet block，每个block含有res_num个residual blocks\n",
    "    - 对一个block，其包含的若干个residual block中，第一个是一个(c_in,c_out)且stride为2的residual block（控制特征数和image size），剩下的是(c_out,c_out)的residual block（提取特征）。\n",
    "      - 一个residual block的输出：内部两个cnn的结构的输出，和输入在最后相加。\n",
    "  - 然后是global average pool：将image size变为1，1\n",
    "  - 然后是flatten+特征维度的fc：nn.Linear(～,10) 这里的feature_num是512（当然，可以更大）\n",
    "\n",
    "- ResNet细节在代码中注释了。\n",
    "  - ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。\n",
    "  - ResNet包含若干个resnet_block，每个resnet_block包含指定num的residual_block，residual_block另定义了类，是一个残差思想的结构。\n",
    "  - 不要忘记glovalAvgPooling以及flattenlayer\n",
    "    - 前者起到fc作用，将image size变为1\\*1\n",
    "    - 后者将image维度去掉，只剩下(batch,feature_num)，用于linear层分类到softmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yv3EpDCpm9Au",
    "outputId": "b6d2316e-46cd-4363-bce2-b0185050a061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vCvjEoKUnB5c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/d2dl_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbqyUWGFoWYW",
    "outputId": "2f0573b6-a402-445c-cae2-2683bcff0cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0+cu101\n",
      "device on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.utils import data as Data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "\n",
    "import d2lzh_pytorch as d2dl\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device on:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Q6xhKYtoYmA",
    "outputId": "84cb9305-7909-4600-861c-6837b1bd459c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resNet(\n",
      "  (block_1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block_2): Sequential(\n",
      "    (0): residualBlock(\n",
      "      (cnn_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (cnn_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cnn_3): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "    (1): residualBlock(\n",
      "      (cnn_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (block_3): Sequential(\n",
      "    (0): residualBlock(\n",
      "      (cnn_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (cnn_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cnn_3): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "    (1): residualBlock(\n",
      "      (cnn_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (block_4): Sequential(\n",
      "    (0): residualBlock(\n",
      "      (cnn_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (cnn_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cnn_3): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "    (1): residualBlock(\n",
      "      (cnn_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (block_5): Sequential(\n",
      "    (0): residualBlock(\n",
      "      (cnn_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (cnn_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cnn_3): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "    (1): residualBlock(\n",
      "      (cnn_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): globalAvgPool()\n",
      "  (resNetBlock_layer): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): residualBlock(\n",
      "        (cnn_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (cnn_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (cnn_3): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
      "      )\n",
      "      (1): residualBlock(\n",
      "        (cnn_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (cnn_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): residualBlock(\n",
      "        (cnn_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (cnn_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (cnn_3): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      )\n",
      "      (1): residualBlock(\n",
      "        (cnn_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (cnn_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): residualBlock(\n",
      "        (cnn_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (cnn_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (cnn_3): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "      )\n",
      "      (1): residualBlock(\n",
      "        (cnn_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (cnn_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): residualBlock(\n",
      "        (cnn_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (cnn_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (cnn_3): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "      )\n",
      "      (1): residualBlock(\n",
      "        (cnn_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (cnn_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (bn_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): globalAvgPool()\n",
      "  )\n",
      "  (fc_layer): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# Load Data\n",
    "# non-default argument follows default argument, has to define non-default value first\n",
    "def load_data_from_mnist(batch_size, resize=None, root=''):\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(transforms.Resize(resize))\n",
    "    trans.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(trans)\n",
    "\n",
    "    train_data = torchvision.datasets.MNIST(root=root,train=True,transform=transform,download=False)\n",
    "    test_data = torchvision.datasets.MNIST(root=root,train=False,transform=transform,download=False)\n",
    "    train_iterator = Data.DataLoader(train_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    test_iterator = Data.DataLoader(test_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "    return train_iterator, test_iterator\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=''):\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "\n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "#train_iterator, test_iterator = load_data_fashion_mnist(batch_size,resize=96)\n",
    "train_iterator, test_iterator = load_data_from_mnist(batch_size,resize=96)\n",
    "\n",
    "# Define Model\n",
    "class globalAvgPool(nn.Module):\n",
    "    # the function of global average pooling is to reduce the image size to (1,1),\n",
    "    # which is convenient to reduce dimension later\n",
    "    def __init__(self):\n",
    "        super(globalAvgPool,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return nn.functional.avg_pool2d(x,x.size()[2:])\n",
    "\n",
    "class residualBlock(nn.Module):\n",
    "    # residual block contains 2 parts: \n",
    "    # the first part is a CNN block: (conv-bn-relu), change the feature_num:(c_in,c_out), image size is (optional) remained.\n",
    "    # the second part is another CNN block:(conv-bn-relu), feature_num and image size are remained the same.\n",
    "    # the third part is an (optioanl) 1*1 conv on the input, change the feature_num of the input X to the same as the output Y (if needed).\n",
    "    # in the forward process: the output of the two cnn block:Y and the input:X is added and fed into relu (both normalized).\n",
    "    # the residual forward: res_out = relu(Y+X). (f_num: c_in-c_out, size: (optional determined by stride))\n",
    "    def __init__(self, c_in, c_out, is_11_cnn=False, std=1):\n",
    "        super(residualBlock,self).__init__()\n",
    "        self.cnn_1 = nn.Conv2d(c_in,c_out,kernel_size=3,stride=std,padding=1)\n",
    "        self.cnn_2 = nn.Conv2d(c_out,c_out,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn_1 = nn.BatchNorm2d(c_out)\n",
    "        self.bn_2 = nn.BatchNorm2d(c_out)\n",
    "        if is_11_cnn:\n",
    "            self.cnn_3 = nn.Conv2d(c_in,c_out,kernel_size=1,stride=std)\n",
    "        else:\n",
    "            self.cnn_3 = None\n",
    "    def forward(self, x):\n",
    "        y = nn.functional.relu(self.bn_1(self.cnn_1(x)))\n",
    "        y = self.bn_2(self.cnn_2(y))\n",
    "        if self.cnn_3:\n",
    "            x = self.cnn_3(x)\n",
    "        return nn.functional.relu(y+x) \n",
    "\n",
    "class resNet(nn.Module):\n",
    "    # the resNet contains the following parts:\n",
    "    # the first part is like GoogLeNet except bn after cnn: 7*7conv-bn-relu-pool\n",
    "  \n",
    "    # the second part is a series of resnet block, one block contains (res_num) residual blocks \n",
    "    # among these residual blocks, the first is (c_in, c_out) with (stride=2 (reduce the image size by half) and 1*1) cnn\n",
    "    # the others are repeated (c_out, c_out)\n",
    "\n",
    "    # the third part is a globalAvgPooling based layer to reduce the image size to 1*1 to replace fc(fnn).\n",
    "    # the fourth part is the linear layer to reduce feature_num and feed into softmax.\n",
    "\n",
    "    def __init__(self):\n",
    "        super(resNet,self).__init__()\n",
    "        block_1 = nn.Sequential(\n",
    "            nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "            nn.BatchNorm2d(64),  # new\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        )\n",
    "        block_2 = self.resnet_block(c_in=64,c_out=64,res_num=2,is_first=True)\n",
    "        block_3 = self.resnet_block(64,128,2)\n",
    "        block_4 = self.resnet_block(128,256,2)\n",
    "        block_5 = self.resnet_block(256,512,2)\n",
    "        avgpool = globalAvgPool()\n",
    "        self.resNet_layer = nn.Sequential(block_1,block_2,block_3,block_4,block_5,avgpool)\n",
    "        # Noted that there should be a globalAvgPool to make image size (1*1) and a flatten to reduce the dimension to 2.\n",
    "        self.fc_layer = nn.Linear(512,10)\n",
    "  \n",
    "    def resnet_block(self, c_in, c_out, res_num, is_first=False):\n",
    "        if is_first:\n",
    "            assert c_in == c_out\n",
    "        block = []\n",
    "        for i in range(res_num):\n",
    "            if i == 0:\n",
    "                block.append(residualBlock(c_in,c_out,is_11_cnn=True,std=2))\n",
    "            else:\n",
    "                block.append(residualBlock(c_out,c_out))  # is_11_cnn=False, stride=1 \n",
    "        return nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.resNetBlock_layer(x)\n",
    "        y = self.fc_layer(y.view(x.shape[0],-1))  # faltten layer\n",
    "        return y\n",
    "\n",
    "net = resNet()\n",
    "print(net)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizor = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhFcjVkXoauo",
    "outputId": "a6c14abf-7a95-4029-f3cc-de47b301922c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cuda\n",
      "testing on: cuda\n",
      "Epoch: 1, Average loss: 0.1284, Average accuracy: 95.90%, Test Accuracy: 97.27%, time: 19.3sec\n",
      "testing on: cuda\n",
      "Epoch: 2, Average loss: 0.0433, Average accuracy: 98.69%, Test Accuracy: 98.51%, time: 19.3sec\n",
      "testing on: cuda\n",
      "Epoch: 3, Average loss: 0.0320, Average accuracy: 99.02%, Test Accuracy: 98.73%, time: 19.3sec\n",
      "testing on: cuda\n",
      "Epoch: 4, Average loss: 0.0270, Average accuracy: 99.17%, Test Accuracy: 98.97%, time: 19.3sec\n",
      "testing on: cuda\n",
      "Epoch: 5, Average loss: 0.0222, Average accuracy: 99.29%, Test Accuracy: 99.23%, time: 19.4sec\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "def evaluate_model(net, test_iterator, device):\n",
    "    net = net.to(device)\n",
    "    print('testing on:', device)\n",
    "    with torch.no_grad():\n",
    "        correct,num_exp = 0.0,0\n",
    "        for X,y in test_iterator:\n",
    "            if isinstance(net, nn.Module):\n",
    "                net.eval()  # eval mode will shut off dropout function\n",
    "                correct += (net(X.to(device)).argmax(1)==y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else: \n",
    "                print('is this your self-defined nn module?? we are not considering GPU if so')\n",
    "                if('is_training' in net.__code__.co_varnames): \n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            num_exp += y.size(0)\n",
    "     \n",
    "    return correct/num_exp*100\n",
    "\n",
    "def train_model(num_epochs, train_iterator, test_iterator, loss_func, optimizor, net, device):\n",
    "    net = net.to(device)\n",
    "    print('training on:', device)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss,total_batch,total_acc,total_num,start_time = 0.0,0,0.0,0,time.time()\n",
    "        for X, y in train_iterator:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = net(X)\n",
    "            loss = loss_func(output,y)\n",
    "            optimizor.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizor.step()\n",
    "            \n",
    "            total_loss += loss.cpu().item()\n",
    "            total_batch += 1\n",
    "            total_acc += (output.argmax(1)==y).sum().cpu().item()\n",
    "            total_num += y.size(0)\n",
    "        \n",
    "        test_acc = evaluate_model(net, test_iterator, device)\n",
    "        print('Epoch: {}, Average loss: {:.4f}, Average accuracy: {:.2f}%, Test Accuracy: {:.2f}%, time: {:.1f}sec' \\\n",
    "              .format(epoch+1, total_loss/total_batch, total_acc/total_num*100, test_acc, time.time()-start_time))\n",
    "\n",
    "train_model(num_epochs,train_iterator,test_iterator,loss_func,optimizor,net,device)\n",
    "        \n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gozYPbN03iaZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "resnet_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
