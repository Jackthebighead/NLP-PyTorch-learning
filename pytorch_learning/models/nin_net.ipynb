{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiN Net with MNIST Dataset\n",
    "\n",
    "`Author: YUAN Yanzhe`\n",
    "\n",
    "- This notebook is a reproduction of the [NiN paper](https://arxiv.org/abs/1312.4400).\n",
    "  - If you want to do parameter fine-tuning, setting hyperparameters on the entrance of the model is recommended.\n",
    "    - e.g. def \\_\\_init\\_\\_(param) \n",
    "- The code runs on Google Colab, GPU mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一些细节**\n",
    "- NiN：Network in Network.\n",
    "  - LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。\n",
    "  - 其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。  \n",
    "  - NiN提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。\n",
    "- 1\\*1卷积\n",
    "  - 一般来说，1\\*1卷积一般的目的是不改变image的size而改变in/out channel即feature_num，可以理解为在feature维度上进行卷积。\n",
    "  - 在NiN Net，1\\*1卷积的目的是替代全连接层，因为它有着更少的参数量，配合Global Average Pooling\n",
    "- NiN结构\n",
    "  - nin_block\n",
    "    - conv-relu-conv-relu-conv-relu: 第一个conv控制feature_num，后面两个都是不改变feature_num的1\\*1conv\n",
    "  - 大体上是:\n",
    "    - 3个：nin_block-pool：nin控制feature_num，pool控制image的size。\n",
    "    - dropout: regularizaiton\n",
    "    - nin_block+global_avgpool: nin降维和分类，avgpool将image的size变为1\\*1\n",
    "- 定义了一个global average pooling 类，前面的1\\*1卷积是对feature数改动而不改动image的size的话，那么这个类才是用来取代全连接层的关键部分吗，它的作用是将任何的image size变为(1,1)。\n",
    "  - 好处是减少参数。\n",
    "  - Intuitively, 假如最后的一层的数据是10个6*6的特征图，global average pooling是将每一张特征图计算所有像素点的均值，输出一个数据值，这样10 个特征图就会输出10个数据点，将这些数据点组成一个1*10的向量的话，就成为一个特征向量，就可以送入到softmax的分类中计算了。\n",
    "  - 如果是全连接层，会将feature_num个feature进行拼接，然后用一个简单的FNN网络输出到10维度进行softmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HMv3HJ4vOwa",
    "outputId": "b722d145-14bb-4eaa-8656-1106173730ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ehX9F-1lvWwN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/d2dl_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVmbvKz9vfZu",
    "outputId": "650a3831-94b2-4991-fede-27f5a41781fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0+cu101\n",
      "device on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.utils import data as Data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "\n",
    "import d2lzh_pytorch as d2dl\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device on:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ETCw2Qptvhur"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load Data\n",
    "# non-default argument follows default argument, has to define non-default value first\n",
    "def load_data_from_mnist(batch_size, resize=None, root=''):\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(transforms.Resize(resize))\n",
    "    trans.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(trans)\n",
    "\n",
    "    train_data = torchvision.datasets.MNIST(root=root,train=True,transform=transform,download=False)\n",
    "    test_data = torchvision.datasets.MNIST(root=root,train=False,transform=transform,download=False)\n",
    "    train_iterator = Data.DataLoader(train_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    test_iterator = Data.DataLoader(test_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "    return train_iterator, test_iterator\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=''):\n",
    "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "\n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "#train_iterator, test_iterator = load_data_fashion_mnist(batch_size,resize=224)\n",
    "train_iterator, test_iterator = load_data_from_mnist(batch_size,resize=224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vO3_WGUvlz1",
    "outputId": "ebe02c2e-f4a6-44b7-c985-cd924c394fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninNet(\n",
      "  (nin_layer): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(384, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (8): globalAvgPool()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "class globalAvgPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(globalAvgPool,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return nn.functional.avg_pool2d(x,x.size()[2:])\n",
    "\n",
    "class ninNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ninNet,self).__init__()\n",
    "        self.nin_layer = nn.Sequential(\n",
    "            self.nin_block(1,96,k_size=11,std=4,pad=0),\n",
    "            nn.MaxPool2d(3,2),\n",
    "            self.nin_block(96,256,5,1,2),  # keep the image size, increase the feature_num\n",
    "            nn.MaxPool2d(3,2),  # reduce image_size by half \n",
    "            self.nin_block(256,384,3,1,1),  # keep the image size, increase the feature_num\n",
    "            nn.MaxPool2d(3,2),  # reduce image_size by half \n",
    "            nn.Dropout(0.5), \n",
    "            self.nin_block(384,10,3,1,1),  # keep the image size, decrease the feature_num\n",
    "            globalAvgPool()  # avg with kernel size(img.h,img,w): (batch,10,1,1)\n",
    "        )\n",
    "        \n",
    "    def nin_block(self, c_in, c_out, k_size, std, pad):\n",
    "        # each nin_block can be regarded as a tiny network in the huge ninNet\n",
    "        block = []\n",
    "        block.append(nn.Conv2d(c_in,c_out,k_size,std,pad))\n",
    "        block.append(nn.ReLU())\n",
    "        block.append(nn.Conv2d(c_out,c_out,1))  # 1*1 conv to replace fc so it looks like a tiny network\n",
    "        block.append(nn.ReLU())\n",
    "        block.append(nn.Conv2d(c_out,c_out,1))\n",
    "        block.append(nn.ReLU())\n",
    "        return nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.nin_layer(x)\n",
    "        y = y.view(x.shape[0],-1)  # ultimately turn 4 dimension to 2 dimension:(batch,10)\n",
    "        return y \n",
    "\n",
    "\n",
    "net = ninNet()\n",
    "print(net)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizor = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKTNu_Wg5ndq",
    "outputId": "9b7ef33d-f14a-47d2-aa0f-324412855afd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cuda\n",
      "testing on: cuda\n",
      "Epoch: 1, Average loss: 2.1091, Average accuracy: 19.04%, Test Accuracy: 46.99%, time: 44.8sec\n",
      "testing on: cuda\n",
      "Epoch: 2, Average loss: 0.8541, Average accuracy: 72.04%, Test Accuracy: 82.92%, time: 44.9sec\n",
      "testing on: cuda\n",
      "Epoch: 3, Average loss: 0.4695, Average accuracy: 86.28%, Test Accuracy: 91.83%, time: 45.2sec\n",
      "testing on: cuda\n",
      "Epoch: 4, Average loss: 0.2938, Average accuracy: 91.67%, Test Accuracy: 94.96%, time: 45.3sec\n",
      "testing on: cuda\n",
      "Epoch: 5, Average loss: 0.2136, Average accuracy: 93.92%, Test Accuracy: 96.01%, time: 44.8sec\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "def evaluate_model(net, test_iterator, device):\n",
    "    net = net.to(device)\n",
    "    print('testing on:', device)\n",
    "    with torch.no_grad():\n",
    "        correct,num_exp = 0.0,0\n",
    "        for X,y in test_iterator:\n",
    "            if isinstance(net, nn.Module):\n",
    "                net.eval()  # eval mode will shut off dropout function\n",
    "                correct += (net(X.to(device)).argmax(1)==y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else: \n",
    "                print('is this your self-defined nn module?? we are not considering GPU if so')\n",
    "                if('is_training' in net.__code__.co_varnames): \n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            num_exp += y.size(0)\n",
    "     \n",
    "    return correct/num_exp*100\n",
    "\n",
    "def train_model(num_epochs, train_iterator, test_iterator, loss_func, optimizor, net, device):\n",
    "    net = net.to(device)\n",
    "    print('training on:', device)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss,total_batch,total_acc,total_num,start_time = 0.0,0,0.0,0,time.time()\n",
    "        for X, y in train_iterator:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = net(X)\n",
    "            loss = loss_func(output,y)\n",
    "            optimizor.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizor.step()\n",
    "            \n",
    "            total_loss += loss.cpu().item()\n",
    "            total_batch += 1\n",
    "            total_acc += (output.argmax(1)==y).sum().cpu().item()\n",
    "            total_num += y.size(0)\n",
    "        \n",
    "        test_acc = evaluate_model(net, test_iterator, device)\n",
    "        print('Epoch: {}, Average loss: {:.4f}, Average accuracy: {:.2f}%, Test Accuracy: {:.2f}%, time: {:.1f}sec' \\\n",
    "              .format(epoch+1, total_loss/total_batch, total_acc/total_num*100, test_acc, time.time()-start_time))\n",
    "\n",
    "train_model(num_epochs,train_iterator,test_iterator,loss_func,optimizor,net,device)\n",
    "        \n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsF98pda6A-J"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nin_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
