{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prerequisite for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tensor Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "torch.int32\n",
      "tensor([[[ 2.8026e-44,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00, -2.2729e+11,  4.5828e-41, -2.2729e+11],\n",
      "         [ 4.5828e-41, -2.2729e+11,  4.5828e-41,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 6.4890e-07,  2.5038e-12,  6.3371e-10,  7.9348e+17,  1.3556e-19],\n",
      "         [ 1.8567e-01,  2.8376e+20,  1.8180e+31, -2.2729e+11,  4.5828e-41],\n",
      "         [-2.2729e+11,  4.5828e-41, -2.2730e+11,  4.5828e-41,  1.4013e-45],\n",
      "         [ 0.0000e+00,  1.3556e-19,  1.8567e-01,  7.5553e+28,  5.2839e-11]],\n",
      "\n",
      "        [[ 1.1429e+33,  1.7226e+22,  1.8040e+28,  3.4740e-12,  0.0000e+00],\n",
      "         [ 0.0000e+00,  1.4013e-45,  0.0000e+00,  8.4118e-31,  1.4013e-45],\n",
      "         [ 1.6144e-41,  4.0473e-11,  3.8519e-34,  0.0000e+00,  1.4013e-45],\n",
      "         [ 2.3511e-38,  5.5144e-31,  2.5250e-29,  3.1799e-32,  3.6902e+19]]])\n",
      "torch.float32\n",
      "tensor([[[1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1],\n",
      "         [1, 1, 1, 1, 1]]], dtype=torch.int32)\n",
      "tensor([[[ 1.9547, -0.7976,  2.2585, -0.9026,  0.0353],\n",
      "         [-0.0308, -0.2454, -0.2459,  0.3513,  2.8093],\n",
      "         [ 0.6589,  0.8764,  1.0723,  1.2422, -0.0781],\n",
      "         [ 0.6692, -2.3272, -1.1311, -0.3881, -0.3065]],\n",
      "\n",
      "        [[ 0.0203,  0.0372, -0.1341,  0.1002, -0.7253],\n",
      "         [-1.6316,  0.1179,  0.7512, -0.2764, -0.0424],\n",
      "         [ 0.0835, -1.5981,  0.1994, -0.5464, -0.5403],\n",
      "         [-0.0623,  1.0397, -0.8618, -0.9318, -1.1354]],\n",
      "\n",
      "        [[-0.6001, -0.2313,  1.1526,  0.8589,  1.2456],\n",
      "         [-2.3406,  1.9569,  0.9825, -1.7043, -0.1800],\n",
      "         [-0.3239,  1.3811, -0.5104,  1.1753, -1.5404],\n",
      "         [ 0.3097, -0.0318, -1.4487,  0.8395, -0.1311]]])\n",
      "tensor([[2.3694e-38, 2.3694e-38, 2.3694e-38],\n",
      "        [2.3694e-38, 2.3694e-38, 2.3694e-38],\n",
      "        [2.3694e-38, 2.3694e-38, 2.3694e-38],\n",
      "        [2.3694e-38, 2.3694e-38, 2.3694e-38],\n",
      "        [2.3694e-38, 2.3694e-38, 2.3694e-38]])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([2, 3, 7, 4, 6, 0, 5, 1, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# create tensor variable\n",
    "\n",
    "# torch.rand(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n",
    "x = torch.empty(3,4,dtype=torch.int32)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "\n",
    "x1 = torch.empty([3,4,5])\n",
    "print(x1)\n",
    "print(x1.dtype)\n",
    "\n",
    "\n",
    "# x=torch.zeros([3,4,5])\n",
    "# x=torch.ones([3,4,5])\n",
    "# x=torch.eye([3,3]) # diagonal=1\n",
    "# x=torch.rand([3,4,5]) # uniform distribution\n",
    "\n",
    "x3 = x.new_ones([3,4,5]) # return the same torch.dtype&torch.device as x\n",
    "x4 = torch.randn_like(x1) # return a standard uniform distribution in the form of x1 (has to be float type or sth)\n",
    "print(x3)\n",
    "print(x4)\n",
    "\n",
    "x5 = torch.Tensor(5,3) # Tensor(*size), tensor(data,)\n",
    "print(x5)\n",
    "\n",
    "x6 = torch.arange(1,10,1) # step\n",
    "# x7 = torch.arange(1,10,2) # pieces_cut\n",
    "x7 = torch.randperm(10) # pieces_cut\n",
    "print(x6)\n",
    "print(x7)\n",
    "\n",
    "# x8 = torch.normal(mean, std)\n",
    "# x9 = torch.uniform(from,to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3, 3, 3])\n",
      "tensor([[2, 2, 2, 2],\n",
      "        [3, 3, 3, 3]])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "[[7 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "tensor([[7, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# directly create a tensor, just like numpy\n",
    "import numpy as np\n",
    "print(torch.tensor([3,3,3,3]))\n",
    "print(torch.tensor(np.array([[2,2,2,2],[3,3,3,3]]))) # create a tensor through numpy.array\n",
    "\n",
    "print(x.shape) # return a tuple\n",
    "print(x.size())\n",
    "\n",
    "# transformation with numpy \n",
    "print(x.numpy())\n",
    "print(torch.from_numpy(x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "\n",
    "- 在PyTorch中，所有operation的赋值都会导致新的变量地址开辟，比如a=a+1\n",
    "- 所以要想keep地址不变，用+=或者operation_的操作\n",
    "- .add()和.add_()都能把两个张量加起来，但.add_是in-place操作，比如x.add_(y)，x+y的结果会存储到原来的x中。Torch里面所有带\"_\"的操作，都是in-place的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 3])\n",
      "tensor([3, 3])\n",
      "size: torch.Size([2])\n",
      "tensor([3., 3.])\n",
      "tensor([2, 1])\n",
      "tensor([3, 3])\n",
      "tensor([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# operations\n",
    "\n",
    "# add\n",
    "x = torch.tensor([1,2])\n",
    "y = torch.tensor([2,1])\n",
    "print(x+y)\n",
    "print(torch.add(x,y))\n",
    "print('size:',x.size())\n",
    "\n",
    "result = torch.empty(2) # according to the size()\n",
    "torch.add(x,y,out=result)\n",
    "print(result)\n",
    "\n",
    "z = y.add(x)\n",
    "print(y) # it doesn't change!\n",
    "z = y.add_(x) # inplace should use add_, it changes y\n",
    "print(y)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "- PyTorch中，index是不改变id的操作，view也是\n",
    "- 对view如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0010, 0.7618, 0.2470],\n",
      "        [0.5468, 0.5794, 0.8021]])\n",
      "tensor([[0.5468, 0.5794, 0.8021]])\n",
      "tensor([0.7618, 0.5794])\n",
      "tensor([[9.8264e-04, 7.6176e-01, 2.4699e-01],\n",
      "        [1.5468e+00, 1.5794e+00, 1.8021e+00]])\n",
      "tensor([[1.5468, 1.5794, 1.8021]])\n",
      "tensor([[9.8264e-04, 7.6176e-01],\n",
      "        [1.5468e+00, 1.5794e+00]])\n",
      "tensor([1.5468, 1.5794, 1.8021])\n",
      "tensor([[ True,  True,  True],\n",
      "        [False, False, False]])\n",
      "tensor([0.0010, 0.7618, 0.2470])\n",
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2]])\n",
      "tensor([[1.5468e+00, 7.6176e-01, 2.4699e-01],\n",
      "        [9.8264e-04, 1.5794e+00, 2.4699e-01]])\n",
      "tensor([[7.6176e-01, 9.8264e-04, 9.8264e-04],\n",
      "        [1.5468e+00, 1.5794e+00, 1.5468e+00]])\n"
     ]
    }
   ],
   "source": [
    "# index\n",
    "from operator import *\n",
    "\n",
    "# the index result shares the same memory with the original data\n",
    "\n",
    "x = torch.rand([2,3])\n",
    "print(x)\n",
    "print(x[1:])\n",
    "print(x[:,1])\n",
    "\n",
    "y = x[1:]\n",
    "y += 1\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# advanced index functions\n",
    "z = torch.index_select(x,1,torch.tensor([0,1]))# the second parameter stands for by row or column, the third parameter stands for the id of the row/column.\n",
    "print(z)\n",
    "\n",
    "z = torch.masked_select(x,x>1)\n",
    "print(z)\n",
    "mask = x.le(1) # less equal, ge: greater equal, gt: greater than, eq: equal\n",
    "print(mask)\n",
    "z = torch.masked_select(x,mask)\n",
    "print(z)\n",
    "\n",
    "print(torch.nonzero(x)) # return the index of nonzero elements\n",
    "\n",
    "print(torch.gather(x,0,torch.tensor([[1,0,0],[0,1,0]])))  # 沿给定轴dim，将输入索引张量index指定位置的值进行聚合。\n",
    "print(torch.gather(x,1,torch.tensor([[1,0,0],[0,1,0]])))  # 沿列，1代表这个位置的值变为这一列的第二个值，0代表这个位置的值变为这一列的第一个值\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([6]) torch.Size([3, 2])\n",
      "tensor([[2, 3, 4],\n",
      "        [5, 6, 7]])\n",
      "tensor([2, 3, 4, 5, 6, 7])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "torch.Size([2, 3]) torch.Size([6]) torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# change the shape\n",
    "# result of view shares data with the original vatiable, but not memory, i.e. id(x)!=id(y)\n",
    "# to return a new one, use reshape(), or x.clone().view()\n",
    "\n",
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "y = x.view(6)\n",
    "z = x.clone().view(-1, 2)  \n",
    "print(x.size(), y.size(), z.size())\n",
    "x += 1\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "\n",
    "y = x.reshape(6)\n",
    "z = x.reshape(-1, 2)  \n",
    "print(x.size(), y.size(), z.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15)\n",
      "tensor([1, 5, 9])\n",
      "tensor([[ 30,  36,  42],\n",
      "        [ 66,  81,  96],\n",
      "        [102, 126, 150]])\n",
      "tensor([[ 30,  36,  42],\n",
      "        [ 66,  81,  96],\n",
      "        [102, 126, 150]])\n",
      "tensor([[1, 4, 7],\n",
      "        [2, 5, 8],\n",
      "        [3, 6, 9]])\n",
      "tensor([[ 0.5884,  1.5458,  1.3129, -2.4955],\n",
      "        [-0.9899, -0.0333, -2.0643,  2.9506],\n",
      "        [ 1.6425, -1.0018, -0.3710, -0.3777],\n",
      "        [ 0.0298, -0.0513,  1.5906, -0.6297]])\n",
      "SVD torch.return_types.svd(\n",
      "U=tensor([[-0.5474, -0.3724, -0.7158,  0.2219],\n",
      "        [-0.3164, -0.7250,  0.5186, -0.3244],\n",
      "        [-0.6964,  0.4364,  0.4236,  0.3810],\n",
      "        [-0.3396,  0.3810, -0.1980, -0.8369]]),\n",
      "S=tensor([2.0563, 0.7839, 0.3961, 0.0579]),\n",
      "V=tensor([[-0.5323,  0.4891, -0.1373, -0.6771],\n",
      "        [-0.4830, -0.8271,  0.1465, -0.2474],\n",
      "        [-0.3921,  0.2710,  0.8109,  0.3396],\n",
      "        [-0.5741,  0.0572, -0.5497,  0.6041]]))\n",
      "tensor([[-0.5474, -0.3724, -0.7158,  0.2219],\n",
      "        [-0.3164, -0.7250,  0.5186, -0.3244],\n",
      "        [-0.6964,  0.4364,  0.4236,  0.3810],\n",
      "        [-0.3396,  0.3810, -0.1980, -0.8369]])\n",
      "SVD torch.return_types.svd(\n",
      "U=tensor([[ 1.0000e+00,  0.0000e+00,  5.9605e-08, -5.9605e-08],\n",
      "        [ 2.9802e-08, -3.5723e-01, -4.0902e-01,  8.3969e-01],\n",
      "        [ 0.0000e+00, -9.3226e-01,  2.1124e-01, -2.9371e-01],\n",
      "        [ 0.0000e+00,  5.7245e-02,  8.8774e-01,  4.5678e-01]]),\n",
      "S=tensor([1.5849e+29, 9.9962e-01, 3.3924e-01, 0.0000e+00]),\n",
      "V=tensor([[ 0.0000e+00,  8.1290e-01,  5.8240e-01,  0.0000e+00],\n",
      "        [ 2.1844e-30, -2.4231e-01,  3.3821e-01,  9.0934e-01],\n",
      "        [ 4.7741e-30, -5.2960e-01,  7.3920e-01, -4.1606e-01],\n",
      "        [-1.0000e+00, -3.0577e-30,  4.2678e-30,  0.0000e+00]]))\n"
     ]
    }
   ],
   "source": [
    "# linear algeba\n",
    "\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "y = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "print(x.trace())\n",
    "print(x.diag())\n",
    "print(torch.mm(x,y))\n",
    "print(x.mm(y))\n",
    "\n",
    "#print(x.dot(y)) # only support 1 dim\n",
    "print(x.t())\n",
    "\n",
    "print(torch.rand(4, 4).inverse()) # 求逆\n",
    "\n",
    "print('SVD',torch.rand(4, 4).svd())\n",
    "\n",
    "print(torch.Tensor(4, 4).inverse()) # 求逆\n",
    "\n",
    "print('SVD',torch.Tensor(4, 4).svd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Mechanism\n",
    "\n",
    "- 若tensor运算的两个变量形状不同，先适当复制元素使这两个Tensor形状相同后再按元素运算\n",
    "- 由于x和y分别是1行2列和3行1列的矩阵，如果要计算x + y，那么x中第一行的2个元素被广播（复制）到了第二行和第三行，而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Broadcast mechanism\n",
    "\n",
    "# \n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Memory overhead\n",
    "# operations will create new memory while index doesn't ()\n",
    "\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before) # False means the id of y is different, indicating that a new memory is used.\n",
    "\n",
    "# solution: write to the original momory of y using index\n",
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before) # True\n",
    "\n",
    "# other solutions:\n",
    "# torch.add(x, y, out=y)\n",
    "# y += x\n",
    "# y.add_(x) # add_ equals +=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n",
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Exchange with numpy on CPU\n",
    "\n",
    "# tensor-numpy\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)\n",
    "\n",
    "\n",
    "# numpy-tensor \n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "b += 1\n",
    "print(a, b)\n",
    "\n",
    "c = torch.tensor(a)\n",
    "print(a, c)\n",
    "\n",
    "\n",
    "# on GPU\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # Tensor on GPU\n",
    "    x = x.to(device)                       # == .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to(), change the data type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "- torch.autograd It is a set of automatic derivation engine specially developed for the convenience of users. It can automatically build the calculation diagram according to the input and forward propagation process, and execute the back propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "None\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7fc0d91ed590>\n",
      "True False\n",
      "tensor(27., grad_fn=<MeanBackward0>) \n",
      " tensor([[27., 27., 27.],\n",
      "        [27., 27., 27.],\n",
      "        [27., 27., 27.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# AutoGrad\n",
    "\n",
    "x = torch.ones(3,3,requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "\n",
    "print(x.is_leaf, y.is_leaf)\n",
    "\n",
    "z = pow(y,2)*3\n",
    "print(z.mean(),'\\n',z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# gradient & backward\n",
    "# backward() auto computes the gradient for all 'requires_grad=True' variables.\n",
    "# .grad to show the gradient of a certain variable in the computation map.\n",
    "\n",
    "x = torch.ones(2,2,requires_grad=True)\n",
    "y = x + 2\n",
    "z = pow(y,2)*3\n",
    "out = z.mean()  # mean(): 1/n*\\sum(x_i)\n",
    "print(out)\n",
    "\n",
    "# back propagation\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# back propagation again, grad will be accumulated \n",
    "out2 = x.sum()  # sum():\\sum(x_i)\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# set grad to zero to avoid gradient accumulation\n",
    "out3 = x.sum()\n",
    "#out3.backward()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()  # this step has to be after setting gradient to zero\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 标量是0阶张量(一个数)，是1*1的；\n",
    "- 向量是一阶张量，是1*n的；\n",
    "- 张量可以给出所有坐标间的关系，是n*n的\n",
    "  \n",
    "  \n",
    "- 避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导\n",
    "- z是个张量，所以在调用backward时需要传入一个和z同形的权重向量进行加权求和得到一个标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n",
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "# can't compute the gradient of a tensor on a tensor\n",
    "# need to change the tensor to scalar, and compute the gradient of a tensor on a scalar\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)\n",
    "\n",
    "\n",
    "\n",
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# add requires_grad\n",
    "\n",
    "a = torch.randn(2,2)\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "\n",
    "# if doesn't require_grad, it will not be backward()\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2 \n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # False\n",
    "print(y3, y3.requires_grad) # True\n",
    "y3.backward()\n",
    "print(x.grad) # suppose 5 but 2 because y2 doesn't backward\n",
    "# y2.backward() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x.data独立于计算图之外\n",
    "- 更改data的值会影响tensor的print，但不会影响grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# change data without influence the backward process\n",
    "\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # still a tensor\n",
    "print(x.data.requires_grad) \n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # only changes the value, no influence on the computation map and gradient propagation\n",
    "\n",
    "y.backward()\n",
    "print(x) \n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
